{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda, int32\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Kernel de redução serial para a soma\n",
    "@cuda.jit\n",
    "def reduce_serial_kernel(d_input, d_output):\n",
    "    tid = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "    stride = cuda.gridDim.x * cuda.blockDim.x\n",
    "    temp_sum = 0\n",
    "\n",
    "    # Cada thread faz a acumulação de seus elementos\n",
    "    for i in range(tid, d_input.size, stride):\n",
    "        temp_sum += d_input[i]\n",
    "    \n",
    "    # Adiciona o resultado ao acumulador global usando atomicAdd para evitar condições de corrida\n",
    "    cuda.atomic.add(d_output, 0, temp_sum)\n",
    "\n",
    "# Função para executar o reduce serial e medir o tempo\n",
    "def run_reduce_serial(size):\n",
    "    # Inicializar o array de entrada no host\n",
    "    h_input = np.ones(size, dtype=np.int32)  # Exemplo: preenche com 1 para facilitar a soma\n",
    "    h_output = np.array([0], dtype=np.int32)\n",
    "\n",
    "    # Para arrays muito pequenos, use a CPU diretamente\n",
    "    if size <= 1000:\n",
    "        start = time.time()\n",
    "        result = np.sum(h_input)\n",
    "        end = time.time()\n",
    "        elapsed_time = (end - start) * 1000  # Tempo em milissegundos\n",
    "        print(f\"Array Size: {size}, Result: {result}, Time (CPU): {elapsed_time:.4f} ms\")\n",
    "        return\n",
    "\n",
    "    # Alocar memória no dispositivo\n",
    "    d_input = cuda.to_device(h_input)\n",
    "    d_output = cuda.to_device(h_output)\n",
    "\n",
    "    # Configuração do kernel\n",
    "    threads_per_block = 512  # Aumentando para melhorar a ocupação\n",
    "    blocks_per_grid = min(1024, (size + threads_per_block - 1) // threads_per_block)\n",
    "\n",
    "    # Medir o tempo de execução\n",
    "    start = time.time()\n",
    "    \n",
    "    # Executar o kernel\n",
    "    reduce_serial_kernel[blocks_per_grid, threads_per_block](d_input, d_output)\n",
    "    \n",
    "    # Sincronizar o dispositivo para garantir que o kernel terminou\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    end = time.time()\n",
    "    elapsed_time = (end - start) * 1000  # Tempo em milissegundos\n",
    "\n",
    "    # Copiar o resultado de volta para o host\n",
    "    h_output = d_output.copy_to_host()\n",
    "\n",
    "    # Exibir o resultado e o tempo de execução\n",
    "    print(f\"Array Size: {size}, Result: {h_output[0]}, Time (GPU): {elapsed_time:.4f} ms\")\n",
    "\n",
    "# Tamanhos do array para o teste\n",
    "array_sizes = [100, 1000, 10000, 100000, 1000000, 10000000]\n",
    "\n",
    "# Executar a redução para cada tamanho de array\n",
    "for size in array_sizes:\n",
    "    run_reduce_serial(size)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
